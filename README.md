Transformer Language Model (GPT-like) – PyTorch Project
Student project inspired by Andrej Karpathy’s deep learning tutorials

Implemented a GPT-like transformer language model for text generation using Python and PyTorch.

Trained the model on the novel “Crime and Punishment” using character-level tokenization.

Implemented core transformer components including self-attention, multi-head attention, feed-forward networks, positional embeddings, and layer normalization.

Developed a full training pipeline including data batching, cross-entropy loss computation, and optimization using the AdamW optimizer.

The model was trained to predict the next character in a sequence and generate new text using probabilistic sampling from the output distribution.

Technologies: Python, PyTorch, Neural Networks, Transformers, Deep Learning
